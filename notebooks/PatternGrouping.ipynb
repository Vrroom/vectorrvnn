{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21dd624-1add-41a5-8dbf-c10deda3de78",
   "metadata": {},
   "source": [
    "# Pattern Grouping Model\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Three components \n",
    "\n",
    "1. Resnet18 to extract visual features.\n",
    "2. UNet to extract structural features (more on that later)\n",
    "3. Positional attributes\n",
    "\n",
    "These 3 features are combined into a 32D feature vector. The architecture is trained on triplets sampled from annotated graphics using max margin loss. **The three components are merged using a final linear layer**.\n",
    "\n",
    "## What do I want to study here?\n",
    "\n",
    "1. Why is there a significant gap in loss over train and validation triplets?\n",
    "2. What are the individual components doing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5b79d-7bda-4ea1-9710-f0f7150b39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorrvnn.interfaces import *\n",
    "from vectorrvnn.trainutils import *\n",
    "from vectorrvnn.data import *\n",
    "from vectorrvnn.utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "opts = Options().parse(testing=[\n",
    "    '--batch_size', '32',\n",
    "    '--checkpoints_dir', '../results',\n",
    "    '--dataroot', '../data/All',\n",
    "    '--embedding_size', '32', \n",
    "    '--load_ckpt', 'pattern_oneof/best_0-770.pth',                          \n",
    "    '--modelcls', 'PatternGrouping',\n",
    "    '--name', 'pattern_oneof',\n",
    "    '--structure_embedding_size', '8',\n",
    "    '--samplercls', 'DiscriminativeSampler',\n",
    "    '--device', 'cuda:1'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905c2d6-ec13-4f3e-a574-0e21fc318023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildModel(opts)\n",
    "data = buildData(opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276b622-3dbb-463e-9485-2ff437f57222",
   "metadata": {},
   "source": [
    "## What are the features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a6d6d-1875-4e35-a116-b2af57b0db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotVisImage (im) : \n",
    "    nodeType = ['ref', 'plus', 'minus']\n",
    "    print(nodeType)\n",
    "    ims = []\n",
    "    for ntype, im_ in zip(nodeType, im) :\n",
    "        numpyIm = im_.detach().cpu().numpy()\n",
    "        numpyIm = np.transpose(numpyIm, (1, 2, 0))\n",
    "        ims.append(numpyIm)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, dpi=150)\n",
    "    ax.imshow(np.concatenate(ims, 1))\n",
    "    plt.show()\n",
    "        \n",
    "trainData, valData, trainDataLoader, valDataLoader = data\n",
    "\n",
    "for trainBatch in trainDataLoader : \n",
    "    break\n",
    "    \n",
    "for valBatch in valDataLoader :\n",
    "    break\n",
    "    \n",
    "tripletviscallback = TripletVisCallback()\n",
    "\n",
    "trainTripletImage = tripletviscallback.visualized_image(trainBatch, dict(mask=None), False)\n",
    "valTripletImage   = tripletviscallback.visualized_image(valBatch  , dict(mask=None), True)\n",
    "\n",
    "print(\"Plotting train triplet\")\n",
    "plotVisImage(trainTripletImage)\n",
    "print(\"Plotting val triplet\")\n",
    "plotVisImage(valTripletImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec11ac-d661-42ef-81aa-9d8550d34d0a",
   "metadata": {},
   "source": [
    "1. The resnet18 branch processes the last column, just the path/group\n",
    "2. The UNet branch processes the middle column. (3, 256, 256) -- UNet --> (8, 256, 256). The output of the UNet is weighted averaged by the bitmap (the first column) to produce an 8D vector. This branch is supposed to capture the global context of the graphic from the perspective of the path/group.\n",
    "3. The positional attribute is just the bounding box normalized by document's coordinates\n",
    "\n",
    "## What are hard examples among validation triplets?\n",
    "\n",
    "I'll accumulate all the validation triplets for which ||ref - minus|| < ||ref - plus|| and visually see if I can spot any pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abddfa-5dc0-4e38-9da4-5b74cf3159b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = TripletInterface(opts, model, trainData, valData)\n",
    "\n",
    "rets = [] \n",
    "model.eval()\n",
    "valDataLoader.reset()\n",
    "with torch.no_grad() :\n",
    "    for batch in tqdm(valDataLoader) : \n",
    "        rets.append(interface.forward(batch))\n",
    "        \n",
    "print('loss = ', avg(map(lambda r : r['loss'], rets)))\n",
    "print('% hard triplets = ', avg(map(lambda r : r['hardpct'], rets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee0a05-5a7f-48f7-94b4-a15638a2edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "allValTriplets = aggregateDict(list(valDataLoader), torch.cat)\n",
    "allValRets = aggregateDict(rets, torch.cat, keys=[('dplus',), ('dminus',)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b461439-4953-4623-a9eb-3f007c3d1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = allValRets['dminus'] <= allValRets['dplus']\n",
    "print(\"# hard triplets: \", mask.sum())\n",
    "for i in range((int(mask.sum()) // 5)) : \n",
    "    print(i)\n",
    "    hardTriplet = tripletviscallback.visualized_image(\n",
    "        allValTriplets, \n",
    "        dict(mask=mask), \n",
    "        True,\n",
    "        i=i\n",
    "    )\n",
    "    plotVisImage(hardTriplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16ee1b-737d-49e0-b066-8accddb2d5dc",
   "metadata": {},
   "source": [
    "### What is the problem?\n",
    "\n",
    "1. Data bugs: Look at images 28, 22, 17, 9, 10, 36. There is definitely something wrong with some of the groups. But okay, out of 42 * 3 = 126 nodes, around 6 nodes display this problem. In my view, this is not substantial.\n",
    "2. Size: Often hard triplets contain paths which are quite small relative to canvas. Examples: 42, 39, 32, 35, 29, 28, 26, 25, 10. This can be solved by giving crops/zooms as features too. Why have I not done this already? Because of bugs in pathfinder. Pathfinder *only* accepts a graphic with a viewbox that coincides with the origin. If there is any path outside the viewbox, pathfinder crashes. The temporary solution is to make a big raster and then crop the required path. Will do that.\n",
    "3. Sampling: This is the most important issue. Currently, I sample triplets such that ||REF - PLUS|| < ||REF - MINUS|| where the distance is the length of the path in the tree. But one can easily construct examples where ||PLUS - MINUS|| < ||REF - PLUS|| < ||REF - MINUS||. In such a case, we just confuse the network. I think a majority of the hard triplets above fit into this last case. Examples: 41, 40, 39, 38, 37, 35, 27, 24, 0, 1, 2. Again, this is easy to solve by sampling triplets under the criteria: ||REF - PLUS|| < ||PLUS - MINUS|| and ||REF - PLUS|| < ||REF - MINUS||\n",
    "\n",
    "With that I think I have addressed the question of why the validation loss is greater than training loss. Now let's see what the different branches do!\n",
    "\n",
    "## What are the individual components doing?\n",
    "\n",
    "Here, I'll study:\n",
    "\n",
    "1. What is the output of the UNet?\n",
    "2. Have I correctly computed the bbox features?\n",
    "3. What if I used just one of the components to perform clustering? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62b959-061b-4eab-9fc4-e29b681fd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm visualizing the norm of the final 8 channel image\n",
    "# Don't get much information.\n",
    "with torch.no_grad() :\n",
    "    for i in [10, 13, 14, 17] : \n",
    "        fig, axes = plt.subplots(1, 2, dpi=70)\n",
    "        im1 = allValTriplets['ref']['im'][i].unsqueeze(0)\n",
    "        unetout = model.unet(im1.to(opts.device)).squeeze()\n",
    "        im1 = im1.squeeze().detach().cpu().numpy().transpose(1,2,0)\n",
    "        im2 = (unetout * unetout).sum(0).detach().cpu().numpy()\n",
    "        axes[0].imshow(im1)\n",
    "        axes[1].imshow(im2)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1384160-8a73-4299-8583-24bbf1dd639d",
   "metadata": {},
   "source": [
    "Really not sure what I learned from this visualization. Maybe the TSNE plots of the embedding will clear things up later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264da06-31cc-4e05-91a3-eb89bed9bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBBox (bbox, ax) : \n",
    "    x, y, w, h = bbox.view(-1).tolist()\n",
    "    X = x + w\n",
    "    Y = y + h\n",
    "    y = 1 - y\n",
    "    Y = 1 - Y\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot([x, X], [y, y], color='blue')\n",
    "    ax.plot([x, X], [Y, Y], color='blue')\n",
    "    ax.plot([x, x], [y, Y], color='blue')\n",
    "    ax.plot([X, X], [y, Y], color='blue')\n",
    "\n",
    "for i in range(10) : \n",
    "    fig, axes = plt.subplots(1, 2, dpi=70)\n",
    "    axes[0].imshow(allValTriplets['ref']['whole'][i].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "    drawBBox(allValTriplets['ref']['bbox'][i], axes[1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78ded4-1d06-4d17-b10e-eda7071fc134",
   "metadata": {},
   "source": [
    "The top left and bottom right bounding box coordinates are processed by a linear layer. Obviously, such a layer can't include the interaction between the x and y dims, i.e. the area attributes. I think that I should pass those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64129d78-2a19-4d40-bc6c-970136d3c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEmbedder(PatternGrouping) :\n",
    "    \n",
    "    def embedding (self, node, **kwargs) : \n",
    "        im = node['im']\n",
    "        bitmap = node['bitmap']\n",
    "        f2 = (self.unet(im) * bitmap).sum(dim=(2, 3)) / (bitmap.sum() + 1e-6)\n",
    "        return f2\n",
    "    \n",
    "class ResnetEmbedder (PatternGrouping) :\n",
    "    \n",
    "    def embedding (self, node, **kwargs) : \n",
    "        whole = node['whole']\n",
    "        f1 = self.conv(whole)\n",
    "        return f1\n",
    "        \n",
    "class BBoxEmbedder (PatternGrouping) : \n",
    "    \n",
    "    def embedding(self, node, **kwargs) :\n",
    "        f3 = node['bbox']\n",
    "        return f3\n",
    "    \n",
    "data2Vis = list(valData)[:20]\n",
    "\n",
    "def visualizeEmbeddings (modelcls) : \n",
    "\n",
    "    embedder = deepcopy(model)\n",
    "    embedder.__class__ = modelcls\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for v in data2Vis : \n",
    "            d = deepcopy(v)\n",
    "            t = embedder.greedyTree(d)\n",
    "            d.initTree(t)\n",
    "            d.initGraphic(d.doc)\n",
    "            pathSets = [d.nodes[n]['pathSet'] for n in d.nodes]\n",
    "            features = [PatternGrouping.nodeFeatures(d, ps, opts) for ps in pathSets]\n",
    "            tensorApply(\n",
    "                features,\n",
    "                lambda x : x.to(opts.device).unsqueeze(0)\n",
    "            )\n",
    "            embeddings = np.concatenate([embedder.embedding(f).detach().cpu().numpy() for f in features])\n",
    "            fig, axes = plt.subplots(1, 1, dpi=100)\n",
    "            treeAxisFromGraph(d, axes)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4bd76-dc49-43e3-820c-47275e19d833",
   "metadata": {},
   "source": [
    "### Hierachies for UNetEmbedder\n",
    "\n",
    "If I look at the trees produced, I don't think there is any difference between what is produced here and what would be produced had I just given the bitmap of the graphic as input. However, in some examples, color seems to be taken into account and such as the Hat, the Wing Lady, the Wine Glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6937b2-8a3d-4f8c-9981-57b226bc396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeEmbeddings(UNetEmbedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a88a1-7104-4f72-aade-5e425f6fc4c6",
   "metadata": {},
   "source": [
    "### Hierachies for ResnetEmbedder\n",
    "\n",
    "The ResNet embedder, on its own is quite good actually. Differentiation between stroke and fill isn't quite accurate. It can detect symmetry such as for the flag of South Korea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1a760-3b7f-4d35-b850-dc42173d8a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeEmbeddings(ResnetEmbedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81caf842-3dbd-439d-ac4d-bfa9f46db7c5",
   "metadata": {},
   "source": [
    "### Hierachies and TSNE plots for BBoxEmbedder\n",
    "\n",
    "Not bad actually. It's just the bounding box. So not going to say much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb3679-ef94-4ab2-aa66-88b6120853ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeEmbeddings(BBoxEmbedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed90bb55-fdb3-4322-bfba-5854b9dab7d3",
   "metadata": {},
   "source": [
    "### Complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f14c95d-2d30-48ca-a764-990dc1e07147",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizeEmbeddings(PatternGrouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770e7e2-5f38-42f3-b8ec-7443a65db434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
