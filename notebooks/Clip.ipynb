{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a41f33-edc3-4f0a-acc5-7cf75d4e6b83",
   "metadata": {},
   "source": [
    "# CLIP for selection suggestions\n",
    "\n",
    "CLIP is a model for comparing natural language with images. In this notebook, I want to check if it can be used as part of our selection UI. This is a qualitative study.\n",
    "\n",
    "The CLIP model outputs embeddings (e_text, e_image) for a (text, image) pair. The dot product between the e_text and e_image is the similarity score. For a given text prompt and a graphic, I'll show the best node in the graphic's tree. I'll start out with ground truth annotations for graphics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c63e1-dd12-4733-ae69-ceb943f9cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA and MODEL\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from vectorrvnn.utils import *\n",
    "from vectorrvnn.data import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = TripletDataset('../data/All/Test')\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f5161-98b5-433e-b0ff-5e88d6c00738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchingNode (text, tree) :\n",
    "    pp_text = clip.tokenize([text]).to(device)\n",
    "    pathSets = [tree.nodes[n]['pathSet'] for n in tree.nodes]\n",
    "    subdocs = [subsetSvg(tree.doc, ps) for ps in pathSets]\n",
    "    rasters = [rasterize(sd, 256, 256) for sd in subdocs]\n",
    "    normalized = [(r - r.min()) / (r.max() - r.min()) for r in rasters]\n",
    "    bit8 = [(n * 255).astype(np.uint8) for n in normalized]\n",
    "    images = [Image.fromarray(b, 'RGBA') for b in bit8]\n",
    "    pp_image = torch.stack([preprocess(im) for im in images]).to(device)\n",
    "    with torch.no_grad() : \n",
    "        logits_per_image, _ = model(pp_image, pp_text)\n",
    "        probs = logits_per_image.softmax(dim=0).cpu().numpy()\n",
    "        probs = probs.reshape(-1)\n",
    "    top3 = probs.argsort()[-3:][::-1]\n",
    "    fig, axes = plt.subplots(1, 3)\n",
    "    print(\"Showing top 3 matches for prompt -\", text)\n",
    "    for i, ax in enumerate(axes) : \n",
    "        ax.imshow(bit8[top3[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f2e619-2f74-4a5c-ad74-1a618cf4fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId = 42\n",
    "\n",
    "print(\"Showing whole graphic\")\n",
    "plt.imshow(rasterize(data[dataId].doc, 256, 256))\n",
    "plt.show()\n",
    "matchingNode(\"Scissors\", data[dataId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ea3d30-8cb5-4c12-abe9-1a1662c9018c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
